## PDF RAG Question Answering

This repository implements a simple Retrieval-Augmented Generation (RAG) system over a PDF document using:

* **LangChain**: for document loading, text splitting, and vector search (Chroma)
* **Groq LLM**: for answer generation via the `ChatGroq` interface
* **LangGraph**: to define a two-step pipeline (retrieve â†’ answer)
* **Streamlit**: to wrap the RAG agent in a web UI

---
![image](https://github.com/user-attachments/assets/49ddada6-ae1e-4dda-b44c-d6cbaea65f50)
---

### ðŸ“‚ Repository Structure

```bash
.
â”œâ”€â”€ main.py            # Streamlit UI application
â”œâ”€â”€ rag_agent.py       # Core RAG pipeline and rag_agent() function
â”œâ”€â”€ embedding.py       # User-defined get_embeddings() function
â”œâ”€â”€ attention.pdf      # Sample PDF document for RAG (replaceable)
â”œâ”€â”€ chroma_langchain_db/  # Persisted Chroma vector database
â””â”€â”€ requirements.txt   # Python dependencies
```

---

### ðŸ› ï¸ Core Components

#### 1. `rag_agent.py`

* **PDF Loader & Splitter**: uses `PyPDFLoader` + `RecursiveCharacterTextSplitter` to turn the PDF into 1,000â€‘token chunks with 200â€‘token overlaps.
* **Vector Store**: builds or loads a Chroma vector store, persisting embeddings to `./chroma_langchain_db`.
* **LangGraph Pipeline**:

  1. **retrieve**: performs similarity search for the top 4 chunks given the userâ€™s question, storing plain text in `state["context"]`.
  2. **answer**: formats a prompt template with context + question, invokes the Groq LLM, and returns the answer.
* **Public Function**: `rag_agent(question: str) -> str` wraps the pipeline to return a simple string.

#### 2. `main.py`

* **Streamlit UI**:

  * Displays which PDF is used.
  * Text input for user questions.
  * "Get Answer" button triggers `rag_agent`.
  * Shows the modelâ€™s answer.

#### 3. `embedding.py`

* Must define `get_embeddings()` which returns a function to compute vector embeddings for text chunks.

